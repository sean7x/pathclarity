{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import altair as alt\n",
    "import vegafusion as vf\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "alt.data_transformers.enable(\"vegafusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-define chart function\n",
    "def chart(df, x, y, title, color=alt.value('steelblue'), width=480, height=320):\n",
    "    return alt.Chart(df).encode(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        color=color,\n",
    "    ).properties(\n",
    "    title=title,\n",
    "    width=width,\n",
    "    height=height,\n",
    ").configure(\n",
    "    axis=alt.AxisConfig(\n",
    "        domain=False, # remove axis line\n",
    "        ticks=False, # remove ticks\n",
    "        labelAngle=0, # rotate labels\n",
    "        labelColor='gray', # color of labels\n",
    "        labelFontSize=10,\n",
    "    ),\n",
    "    font='Helvetica Neue',\n",
    "    view=alt.ViewConfig(stroke=None), # remove border\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join('..', 'data', 'cleaned')\n",
    "\n",
    "train_df = pd.read_parquet(os.path.join(input_path, 'train.parquet'))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load the vairiables dictionary and define features for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables dictionary\n",
    "with open(os.path.join(input_path, 'variables.json'), 'r') as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "print(f'Variable Categories:\\n')\n",
    "for category, list in variables.items():\n",
    "    print(f'{category}')\n",
    "    print(f'{list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Defining features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### !!! The statistical test result of the features should be referred first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[variables['visitReason']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the independent variables as features for classification\n",
    "features = \\\n",
    "    ['AGE', 'AGER', 'SEX', 'USETOBAC'] + variables['visitReason'] + ['PASTVIS'] + variables['vitalSigns'] \\\n",
    "    + variables['presentSymptomsStatus'] + variables['textFeature']\n",
    "\n",
    "print(f'Features: {features}')\n",
    "print(f'Number of Features: {len(features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.loc[:, features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Preprocess and engineer the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.1 - Bin the Reason for Visit variables into Modules\n",
    " RFV1, RFV2, RFV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the REASON FOR VISIT classification summary of codes\n",
    "rfv_summary = pd.read_excel(os.path.join('..', 'data', 'raw', 'RFV_codes_summary.xlsx'))\n",
    "\n",
    "# Split the 'CODE NUMBER' column into 'START' and 'END' columns\n",
    "rfv_summary[['START', 'END']] = rfv_summary['CODE NUMBER'].str.split('-', expand=True).astype(int)\n",
    "\n",
    "# Remove the leading and trailing whitespaces from `MODULE_1` and `MODULE_2` columns\n",
    "rfv_summary['MODULE_1'] = rfv_summary['MODULE_1'].str.strip()\n",
    "rfv_summary['MODULE_2'] = rfv_summary['MODULE_2'].str.strip()\n",
    "\n",
    "rfv_summary.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the `START` and `END` range, \n",
    "# and map the corresponding `MODULE_1` and `MODULE_2` to X_train as new columns `MODULE_1` and `MODULE_2`,\n",
    "# according to the value of `RFV1`, `RFV2`, and `RFV3` columns\n",
    "def get_module(code):\n",
    "    module = rfv_summary.loc[(rfv_summary['START'] <= code) & (rfv_summary['END'] >= code), ['MODULE_1', 'MODULE_2']]\n",
    "    if len(module) == 0:\n",
    "        return pd.Series([pd.NA, pd.NA], index=['MODULE_1', 'MODULE_2'])\n",
    "    else:\n",
    "        return module.iloc[0]\n",
    "\n",
    "\n",
    "\n",
    "X_train[['RFV1_MOD1', 'RFV1_MOD2']] = X_train['RFV1'].apply(lambda x: get_module(int(str(x)[:4])) if pd.notna(x) else pd.Series([pd.NA, pd.NA], index=['MODULE_1', 'MODULE_2']))\n",
    "print(f'RFV1 unique values: \\n{X_train[\"RFV1_MOD2\"].value_counts()}')\n",
    "\n",
    "X_train[['RFV2_MOD1', 'RFV2_MOD2']] = X_train['RFV2'].apply(lambda x: get_module(int(str(x)[:4])) if pd.notna(x) else pd.Series([pd.NA, pd.NA], index=['MODULE_1', 'MODULE_2']))\n",
    "print(f'RFV2 unique values: \\n{X_train[\"RFV2_MOD2\"].value_counts()}')\n",
    "\n",
    "X_train[['RFV3_MOD1', 'RFV3_MOD2']] = X_train['RFV3'].apply(lambda x: get_module(int(str(x)[:4])) if pd.notna(x) else pd.Series([pd.NA, pd.NA], index=['MODULE_1', 'MODULE_2']))\n",
    "print(f'RFV3 unique values: \\n{X_train[\"RFV3_MOD2\"].value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Binning of quantitative variables to categorical features\n",
    "Bin the following quantitative variables:\n",
    "\n",
    "AGE, BMI, TEMPF, BPSYS, BPDIAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 - Bin the AGE variable\n",
    "Do we bin as recoded age groups (`AGER`) or as each 20 years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of `AGER`\n",
    "# 1 = Under 15 years \n",
    "# 2 = 15-24 years \n",
    "# 3 = 25-44 years \n",
    "# 4 = 45-64 years \n",
    "# 5 = 65-74 years \n",
    "# 6 = 75 years and over|\n",
    " \n",
    "chart(\n",
    "    df=train_df,\n",
    "    x='AGER:O',\n",
    "    y='count()',\n",
    "    title='Distribution of AGER',\n",
    ").mark_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the AGE variable as age groups\n",
    "# 0-2 = Infant\n",
    "# 2-4 = Toddler\n",
    "# 4-12 = Child\n",
    "# 12-20 = Teenager\n",
    "# 20-40 = Adult\n",
    "# 40-60 = Middle Aged\n",
    "# >= 60 = Senior\n",
    "\n",
    "age_groups = ['Infant', 'Toddler', 'Child', 'Teenager', 'Child or Teenager', 'Adult', 'Middle Aged', 'Senior']\n",
    "\n",
    "def bin_age(age):\n",
    "    if pd.isna(age): return pd.NA\n",
    "    #if age < 2: return 'Infant'\n",
    "    #elif age < 4: return 'Toddler'\n",
    "    #elif age < 12: return 'Child'\n",
    "    #elif age < 20: return 'Teenager'\n",
    "    elif age < 20: return 'Child or Teenager'\n",
    "    elif age < 40: return 'Adult'\n",
    "    elif age < 60: return 'Middle Aged'\n",
    "    else: return 'Senior'\n",
    "    \n",
    "\n",
    "X_train['AGE_GROUP'] = X_train['AGE'].apply(bin_age)\n",
    "\n",
    "# Check the distribution of age groups\n",
    "chart(\n",
    "    df=X_train,\n",
    "    x=alt.X('AGE_GROUP:O', sort=age_groups),\n",
    "    y='count()',\n",
    "    title='Distribution of AGE GROUPS',\n",
    ").mark_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 - Bin the vitalSigns variables\n",
    "BMI, TEMPF, BPSYS, BPDIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the BMI as weight status\n",
    "# <18.5 = Underweight\n",
    "# 18.5-25 = Normal weight\n",
    "# 25-30 = Overweight\n",
    "# >=30 = Obesity\n",
    "\n",
    "bmi_groups = ['Underweight', 'Normal weight', 'Overweight', 'Obesity']\n",
    "\n",
    "def bin_bmi(bmi):\n",
    "    if pd.isna(bmi): return pd.NA\n",
    "    elif bmi < 18.5: return 'Underweight'\n",
    "    elif bmi < 25: return 'Normal weight'\n",
    "    elif bmi < 30: return 'Overweight'\n",
    "    else: return 'Obesity'\n",
    "\n",
    "X_train['BMI_GROUP'] = X_train['BMI'].apply(bin_bmi)\n",
    "\n",
    "# Check the distribution of BMI groups\n",
    "chart(\n",
    "    df=X_train,\n",
    "    x=alt.X('BMI_GROUP:O', sort=bmi_groups),\n",
    "    y='count()',\n",
    "    title='Distribution of BMI GROUPS',\n",
    ").mark_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the TEMPF as fever status\n",
    "# <95 = Hypothermia\n",
    "# 95-99 = Normal temperature\n",
    "# 99-100 = Low grade fever\n",
    "# 100-103 = Fever\n",
    "# >=103 = Hyperpyrexia\n",
    "\n",
    "tempf_groups = ['Hypothermia', 'Normal temperature', 'Low grade fever', 'Fever', 'Hyperpyrexia']\n",
    "\n",
    "def bin_tempf(tempf):\n",
    "    if pd.isna(tempf): return pd.NA\n",
    "    elif tempf < 95: return 'Hypothermia'\n",
    "    elif tempf < 99: return 'Normal temperature'\n",
    "    #elif tempf < 100: return 'Low grade fever'\n",
    "    elif tempf < 103: return 'Fever'\n",
    "    else: return 'Hyperpyrexia'\n",
    "\n",
    "X_train['TEMPF_GROUP'] = X_train['TEMPF'].apply(bin_tempf)\n",
    "\n",
    "# Check the distribution of TEMPF groups\n",
    "chart(\n",
    "    df=X_train,\n",
    "    x=alt.X('TEMPF_GROUP:O', sort=tempf_groups),\n",
    "    y='count()',\n",
    "    title='Distribution of TEMPF GROUPS',\n",
    ").mark_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the BPSYS as systolic blood pressure status\n",
    "# <90 = Hypotension\n",
    "# 90-120 = Normal blood pressure\n",
    "# 120-140 = Prehypertension\n",
    "# >=140 = Hypertension\n",
    "\n",
    "bpsys_groups = ['Hypotension', 'Normal blood pressure', 'Prehypertension', 'Hypertension']\n",
    "\n",
    "def bin_bpsys(bpsys):\n",
    "    if pd.isna(bpsys): return pd.NA\n",
    "    elif bpsys < 90: return 'Hypotension'\n",
    "    elif bpsys < 120: return 'Normal blood pressure'\n",
    "    elif bpsys < 140: return 'Prehypertension'\n",
    "    else: return 'Hypertension'\n",
    "\n",
    "X_train['BPSYS_GROUP'] = X_train['BPSYS'].apply(bin_bpsys)\n",
    "\n",
    "# Check the distribution of BPSYS groups\n",
    "chart(\n",
    "    df=X_train,\n",
    "    x=alt.X('BPSYS_GROUP:O', sort=bpsys_groups),\n",
    "    y='count()',\n",
    "    title='Distribution of BPSYS GROUPS',\n",
    ").mark_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the BPDIAS as diastolic blood pressure status\n",
    "# <60 = Low diastolic blood pressure\n",
    "# 60-90 = Normal diastolic blood pressure\n",
    "# 90-110 = High diastolic blood pressure\n",
    "# >=110 = Hypertension\n",
    "\n",
    "bpdias_groups = [\n",
    "    'Low diastolic blood pressure', 'Normal diastolic blood pressure', 'High diastolic blood pressure', 'Hypertension'\n",
    "]\n",
    "\n",
    "def bin_bpdias(bpdias):\n",
    "    if pd.isna(bpdias): return pd.NA\n",
    "    elif bpdias < 60: return 'Low diastolic blood pressure'\n",
    "    elif bpdias < 90: return 'Normal diastolic blood pressure'\n",
    "    elif bpdias < 110: return 'High diastolic blood pressure'\n",
    "    else: return 'Hypertension'\n",
    "\n",
    "X_train['BPDIAS_GROUP'] = X_train['BPDIAS'].apply(bin_bpdias)\n",
    "\n",
    "# Check the distribution of BPDIAS groups\n",
    "chart(\n",
    "    df=X_train,\n",
    "    x=alt.X('BPDIAS_GROUP:O', sort=bpdias_groups),\n",
    "    y='count()',\n",
    "    title='Distribution of BPDIAS GROUPS',\n",
    ").mark_bar().configure_axisX(labelAngle=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Create interaction features\n",
    "AGE, HTIN, WTLB, BMI, BPSYS, BPDIAS, CEBVD, CHF, DIABETES, HYPLIPID, HTN, OBESITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Redefine the features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = [feature for feature in features if X_train[feature].nunique() == 2]\n",
    "print(f'Binary Features: {binary_features}')\n",
    "\n",
    "ordinal_features = ['CASTAGE']\n",
    "print(f'Ordinal Features: {ordinal_features}')\n",
    "print()\n",
    "\n",
    "# With Binned Groups\n",
    "quantitative_features_w_bin = ['PASTVIS', 'HTIN', 'WTLB']\n",
    "print(f'Quantitative Features with Binned Groups: {quantitative_features_w_bin}')\n",
    "\n",
    "nominal_features_w_bin = ['INJDET', 'MAJOR'] + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2'] + ['AGE_GROUP', 'BMI_GROUP', 'TEMPF_GROUP', 'BPSYS_GROUP', 'BPDIAS_GROUP']\n",
    "print(f'Nominal Features with Binned Groups: {nominal_features_w_bin}')\n",
    "\n",
    "print(f'Number of Features with Binned Groups: {len(quantitative_features_w_bin + binary_features + ordinal_features + nominal_features_w_bin)}')\n",
    "print()\n",
    "\n",
    "# Without Binned Groups\n",
    "quantitative_features_wo_bin = ['AGE', 'PASTVIS', 'HTIN', 'WTLB', 'BMI', 'TEMPF', 'BPSYS', 'BPDIAS']\n",
    "print(f'Quantitative Features without Binned Groups: {quantitative_features_wo_bin}')\n",
    "nominal_features_wo_bin = ['INJDET', 'MAJOR'] + ['RFV1', 'RFV2', 'RFV3']\n",
    "print(f'Nominal Features without Binned Groups: {nominal_features_wo_bin}')\n",
    "\n",
    "print(f'Number of Features without Binned Groups: {len(quantitative_features_wo_bin + binary_features + ordinal_features + nominal_features_wo_bin)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Handeling missing values in categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missing values in X_train\n",
    "print(f'Missing Values in X_train: \\n{X_train.isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "\n",
    "# Fill the missing values in the categorical features\n",
    "# with -9 for 'CASTAGE',\n",
    "# with -999 for 'USETOBAC', 'INJDET', 'MAJOR',\n",
    "# with -9 for 'RFV1', 'RFV2', 'RFV3'\n",
    "# with 'NA' for 'RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1', 'RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2',\n",
    "# with 'NA' for 'BMI_GROUP', 'TEMPF_GROUP', 'BPSYS_GROUP', 'BPDIAS_GROUP'\n",
    "X_train.fillna({'CASTAGE': -9}, inplace=True)\n",
    "X_train.fillna({'USETOBAC': -999, 'INJDET': -999, 'MAJOR': -999}, inplace=True)\n",
    "X_train.fillna({'RFV1': -9, 'RFV2': -9, 'RFV3': -9}, inplace=True)\n",
    "X_train.fillna(\n",
    "    {\n",
    "        'RFV1_MOD1': 'NA', 'RFV2_MOD1': 'NA', 'RFV3_MOD1': 'NA',\n",
    "        'RFV1_MOD2': 'NA', 'RFV2_MOD2': 'NA', 'RFV3_MOD2': 'NA',\n",
    "        'BMI_GROUP': 'NA', 'TEMPF_GROUP': 'NA', 'BPSYS_GROUP': 'NA', 'BPDIAS_GROUP': 'NA'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "# Check the missing values in X_train after filling\n",
    "print(f'Missing Values in X_train after Filling: \\n{X_train.isna().sum().where(lambda x: x > 0).dropna()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Prepare dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missing values in 'DIAG1', 'DIAG2', and 'DIAG3'\n",
    "print(f'Missing Values in DIAG1: {train_df[\"DIAG1\"].isna().sum()}')\n",
    "print(f'Missing Values in DIAG2: {train_df[\"DIAG2\"].isna().sum()}')\n",
    "print(f'Missing Values in DIAG3: {train_df[\"DIAG3\"].isna().sum()}')\n",
    "print()\n",
    "\n",
    "# Check the numbers of ruled out or questionable diagnoses\n",
    "# (when 'PRDIAG1', 'PRDIAG2', and 'PRDIAG3' equals to 1)\n",
    "print(f'Number of Ruled Out Diagnoses in DIAG1: {train_df[\"PRDIAG1\"].sum()}')\n",
    "print(f'Number of Ruled Out Diagnoses in DIAG2: {train_df[\"PRDIAG2\"].sum()}')\n",
    "print(f'Number of Ruled Out Diagnoses in DIAG3: {train_df[\"PRDIAG3\"].sum()}')\n",
    "print()\n",
    "\n",
    "# Check the number of samples with missing 'DIAG1' and 'PRDIAG1' equals to 1\n",
    "print(f'Number of Samples with Missing DIAG1 and PRDIAG1 equals to 1: {train_df[(train_df[\"DIAG1\"].isna()) & (train_df[\"PRDIAG1\"] == 1)].shape[0]}')\n",
    "print()\n",
    "\n",
    "# Check the number of available dependent samples\n",
    "# (when 'DIAG1' is not missing and 'PRDIAG1' is not 1)\n",
    "print(f'Number of Available Dependent Samples: {train_df[(~train_df[\"DIAG1\"].isna()) & (train_df[\"PRDIAG1\"] != 1)].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Load and the list of three-digit categories of ICD-9-CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of three-digit categories of ICD-9-CM\n",
    "icd9cm_3dcat = pd.read_excel(os.path.join('..', 'data', 'raw', 'ICD9CM_3DCat.xlsx'), dtype=str)\n",
    "\n",
    "icd9cm_3dcat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Employing the hierachical classifications of ICD-9-CM codes to prepare the target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the three-digit categories of ICD-9-CM to 'DIAG1', 'DIAG2', and 'DIAG3',\n",
    "# if 'PRDIAG1', 'PRDIAG2', and 'PRDIAG3' are not 1 respectively\n",
    "\n",
    "def get_icd9cm_3dcat(diag, prdiag, category='CATEGORY_1'):\n",
    "    try:\n",
    "        if pd.notna(diag) and (pd.isna(prdiag) | prdiag != 1):\n",
    "            if diag == 'V997-':\n",
    "                return 'No diagnosis/disease or healthy'\n",
    "            else:\n",
    "                return icd9cm_3dcat[icd9cm_3dcat['3D_CODE'] == diag[:3]][category].values[0]\n",
    "        else:\n",
    "            return pd.NA\n",
    "    except:\n",
    "        print(f'Error: {diag}')\n",
    "        print(f'Error: {prdiag}')\n",
    "\n",
    "    \n",
    "get_icd9cm_3dcat(train_df.iloc[0].DIAG1, train_df.iloc[0].PRDIAG1, category='CATEGORY_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the three-digit categories of ICD-9-CM to 'DIAG1', 'DIAG2', and 'DIAG3',\n",
    "# if 'PRDIAG1', 'PRDIAG2', and 'PRDIAG3' are not 1 respectively\n",
    "\n",
    "y_train = train_df.apply(lambda x: get_icd9cm_3dcat(x.DIAG1, x.PRDIAG1, category='CATEGORY_1'), axis=1)\n",
    "y_train_cat2 = train_df.apply(lambda x: get_icd9cm_3dcat(x.DIAG1, x.PRDIAG1, category='CATEGORY_2'), axis=1)\n",
    "\n",
    "print(f'Dependent DataFrame with CATEGORY_1 Shape: {y_train.shape}')\n",
    "print(f'Dependent DataFrame with CATEGORY_2 Shape: {y_train_cat2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Drop the rows from both X_train, y_train with NA in y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of available dependent samples: {y_train.notna().sum()}')\n",
    "print()\n",
    "\n",
    "non_missing_mask = y_train.notna()\n",
    "\n",
    "X_train = X_train.loc[non_missing_mask]\n",
    "\n",
    "y_train = y_train.loc[non_missing_mask]\n",
    "y_train_cat2 = y_train_cat2[non_missing_mask]\n",
    "\n",
    "print(f'X_train Shape: {X_train.shape}')\n",
    "print(f'y_train with CATEGORY_1 Shape: {y_train.shape}')\n",
    "print(f'y_train with CATEGORY_2 Shape: {y_train_cat2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Add in text feature\n",
    "From 'RFV1_TEXT', 'RFV2_TEXT', 'RFV3_TEXT'\n",
    "\n",
    "From combined textual feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 - Combine and preprocess textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'RFV1_TEXT', 'RFV2_TEXT', 'RFV3_TEXT' into 'TEXT'\n",
    "X_train['TEXT'] = X_train['RFV1_TEXT'].fillna('') + ' ' + X_train['RFV2_TEXT'].fillna('') + ' ' + X_train['RFV3_TEXT'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text features with Spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "X_train['TEXT'] = X_train['TEXT'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 - Generate text feature with LDA (Topic probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_features(df, n_topics=10, n_top_words=10, transform=False, random_state=random_state):\n",
    "    \"\"\"Generate topic features (topic probabilities) from text features using LDA.\"\"\"\n",
    "    # Define the count vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        #stop_words='english',\n",
    "        ngram_range=(1, 1),\n",
    "        max_features=1000,\n",
    "        min_df=1,\n",
    "        max_df=0.95,\n",
    "    )\n",
    "    tf = vectorizer.fit_transform(df['TEXT'])\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, learning_method='batch', n_jobs=-1, random_state=random_state)\n",
    "    lda.fit(tf)\n",
    "\n",
    "    # Define the function to display the top words for each topic\n",
    "    def display_topics(model, feature_names, n_top_words):\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            print(f'Topic {topic_idx}:')\n",
    "            print(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "            print()\n",
    "    \n",
    "    display_topics(lda, vectorizer.get_feature_names_out(), n_top_words)\n",
    "\n",
    "    # Define the topic features\n",
    "    topics = lda.transform(tf)\n",
    "    topic_features = [f'TOPIC_{i}' for i in range(topics.shape[1])]\n",
    "    print(f'Topic Features: {topic_features}')\n",
    "    topics = pd.DataFrame(topics, columns=topic_features, index=df.index)\n",
    "\n",
    "    # Transform the topic features with PowerTransformer or Log transformation\n",
    "    if transform == 'power':\n",
    "        topics = np.sqrt(topics)\n",
    "    elif transform == 'log':\n",
    "        topics = np.log(topics + 0.0001)\n",
    "\n",
    "    # Combine the topic features with df\n",
    "    df = pd.concat([df, topics], axis=1)\n",
    "    print(f'DataFrame Shape: {df.shape}')\n",
    "    return df, vectorizer, tf, lda, topic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, vectorizer, tf, lda, topic_features = generate_topic_features(\n",
    "    X_train, n_topics=10, n_top_words=10, transform='log'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 - Visualize the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics with pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.lda_model.prepare(lda, tf, vectorizer, mds='tsne')\n",
    "lda_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the pipeline\n",
    "def set_pipeline(model, binary_features, ordinal_features, nominal_features, quantitative_features, imputer=None, n_neighbors=5, pca=False, ovr=False):\n",
    "    # Define the column transformer for the independent variables\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('binary', 'passthrough', binary_features),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), nominal_features),\n",
    "            ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-9), ordinal_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if imputer == 'knn':\n",
    "        preprocessor.transformers.insert(1, ('impute', KNNImputer(n_neighbors=n_neighbors), quantitative_features))\n",
    "\n",
    "    if ovr:\n",
    "        model = OneVsRestClassifier(model)\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', model),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if pca:\n",
    "        pipeline.steps.insert(2, ('pca', PCA()))\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 - Logistic Regression - Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1.1.1 - Logistic Regression - Dropping the quantitative columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Binned Groups and RFV Modules\n",
    "quantitative_features = quantitative_features_w_bin\n",
    "nominal_features = nominal_features_w_bin\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features\n",
    "\n",
    "# Drop the quantitative columns with missing values\n",
    "print(f'Dropping columns with missing values: {set(X_train[features].columns) - set(X_train[features].dropna(axis=1).columns)}')\n",
    "quantitative_features = [feature for feature in quantitative_features if feature in X_train[features].dropna(axis=1).columns]\n",
    "\n",
    "print()\n",
    "print(f'Shape of X_train after dropping columns with missing values: {X_train[features].dropna(axis=1).shape}')\n",
    "print(f'Features after dropping columns with missing values: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "pipeline = set_pipeline(\n",
    "    model=LogisticRegression(\n",
    "        random_state=random_state, n_jobs=-1,\n",
    "        max_iter=1000, class_weight='balanced'\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With original quantitative data and RFV codes\n",
    "quantitative_features = quantitative_features_wo_bin\n",
    "nominal_features = nominal_features_wo_bin\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features\n",
    "\n",
    "# Drop the quantitative columns with missing values\n",
    "print(f'Dropping columns with missing values: {set(X_train[features].columns) - set(X_train[features].dropna(axis=1).columns)}')\n",
    "quantitative_features = [feature for feature in quantitative_features if feature in X_train[features].dropna(axis=1).columns]\n",
    "print()\n",
    "\n",
    "print(f'Shape of X_train after dropping columns with missing values: {X_train[features].dropna(axis=1).shape}')\n",
    "print(f'Features after dropping columns with missing values: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "pipeline = set_pipeline(\n",
    "    model=LogisticRegression(\n",
    "        random_state=random_state, n_jobs=-1,\n",
    "        max_iter=1000, class_weight='balanced'\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1.1.2 - Logistic Regression - Imputing the missing values - KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With original quantitative data and RFV codes\n",
    "quantitative_features = quantitative_features_wo_bin\n",
    "nominal_features = nominal_features_wo_bin\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=LogisticRegression(\n",
    "        random_state=random_state, n_jobs=-1,\n",
    "        max_iter=1000, class_weight='balanced'\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    "    #scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Binned Groups and RFV codes\n",
    "quantitative_features = ['PASTVIS', 'HTIN', 'WTLB']\n",
    "nominal_features = ['INJDET', 'MAJOR', 'RFV1', 'RFV2', 'RFV3', 'AGE_GROUP', 'BMI_GROUP', 'TEMPF_GROUP', 'BPSYS_GROUP', 'BPDIAS_GROUP']\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=LogisticRegression(\n",
    "        random_state=random_state, n_jobs=-1,\n",
    "        max_iter=1000, class_weight='balanced'\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    "    #scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Binned Groups and RFV codes + RFV Modules\n",
    "quantitative_features = ['PASTVIS', 'HTIN', 'WTLB']\n",
    "nominal_features = ['INJDET', 'MAJOR', 'RFV1', 'RFV2', 'RFV3', 'AGE_GROUP', 'BMI_GROUP', 'TEMPF_GROUP', 'BPSYS_GROUP', 'BPDIAS_GROUP']\n",
    "nominal_features = nominal_features + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2']\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=LogisticRegression(\n",
    "        random_state=random_state, n_jobs=-1,\n",
    "        max_iter=1000, class_weight='balanced'\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    "    #scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Binned Groups and RFV codes + RFV Modules + Topic Features\n",
    "quantitative_features = ['PASTVIS', 'HTIN', 'WTLB']\n",
    "nominal_features = ['INJDET', 'MAJOR', 'RFV1', 'RFV2', 'RFV3', 'AGE_GROUP', 'BMI_GROUP', 'TEMPF_GROUP', 'BPSYS_GROUP', 'BPDIAS_GROUP']\n",
    "nominal_features = nominal_features + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2']\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features + topic_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=LogisticRegression(\n",
    "        random_state=random_state, n_jobs=-1,\n",
    "        max_iter=1000, class_weight='balanced'\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    "    #scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With original quantitative data and RFV codes + RFV Modules + Topic Features\n",
    "quantitative_features = quantitative_features_wo_bin\n",
    "nominal_features = nominal_features_wo_bin\n",
    "nominal_features = nominal_features + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2']\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features + topic_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=LogisticRegression(\n",
    "        random_state=random_state, n_jobs=-1,\n",
    "        max_iter=1000, class_weight='balanced'\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    "    #scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Binned Groups and RFV codes + RFV Modules\n",
    "quantitative_features = ['PASTVIS', 'HTIN', 'WTLB']\n",
    "nominal_features = ['INJDET', 'MAJOR', 'RFV1', 'RFV2', 'RFV3', 'AGE_GROUP', 'BMI_GROUP', 'TEMPF_GROUP', 'BPSYS_GROUP', 'BPDIAS_GROUP']\n",
    "nominal_features = nominal_features + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2']\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        criterion='entropy',\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    "    #scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Binned Groups and RFV codes + RFV Modules + Topic Features\n",
    "quantitative_features = ['PASTVIS', 'HTIN', 'WTLB']\n",
    "nominal_features = ['INJDET', 'MAJOR', 'RFV1', 'RFV2', 'RFV3', 'AGE_GROUP', 'BMI_GROUP', 'TEMPF_GROUP', 'BPSYS_GROUP', 'BPDIAS_GROUP']\n",
    "nominal_features = nominal_features + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2']\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features + topic_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        criterion='entropy',\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    "    #scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With original quantitative data and RFV codes + RFV Modules + Topic Features\n",
    "quantitative_features = quantitative_features_wo_bin\n",
    "nominal_features = nominal_features_wo_bin\n",
    "nominal_features = nominal_features + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2']\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features + topic_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        criterion='entropy',\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    "    #scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 - Multi-layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With original quantitative data and RFV codes + RFV Modules + Topic Features\n",
    "quantitative_features = quantitative_features_wo_bin\n",
    "nominal_features = nominal_features_wo_bin\n",
    "nominal_features = nominal_features + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2']\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features + topic_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=MLPClassifier(\n",
    "        #hidden_layer_sizes=(100, 50),\n",
    "        #activation='tanh',\n",
    "        #solver='adam',\n",
    "        #alpha=0.0001,\n",
    "        #max_iter=1000,\n",
    "        random_state=random_state\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=2, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    "    #scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.4 - Histogram-based Gradient Boosting Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With original quantitative data and RFV codes + RFV Modules + Topic Features\n",
    "quantitative_features = quantitative_features_wo_bin\n",
    "nominal_features = nominal_features_wo_bin\n",
    "nominal_features = nominal_features + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2']\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features + topic_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=HistGradientBoostingClassifier(\n",
    "        loss='log_loss',\n",
    "        learning_rate=0.1,\n",
    "        max_iter=100,\n",
    "        max_leaf_nodes=31,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0,\n",
    "        #max_features=1.0,\n",
    "        max_bins=255,\n",
    "        #categorical_features=binary_features + ordinal_features + nominal_features,\n",
    "        #interaction_cst='pairwise',\n",
    "        random_state=random_state,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    "    #scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.4 - Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Binned Groups and RFV codes + RFV Modules\n",
    "quantitative_features = ['PASTVIS', 'HTIN', 'WTLB']\n",
    "nominal_features = ['INJDET', 'MAJOR', 'RFV1', 'RFV2', 'RFV3', 'AGE_GROUP', 'BMI_GROUP', 'TEMPF_GROUP', 'BPSYS_GROUP', 'BPDIAS_GROUP']\n",
    "nominal_features = nominal_features + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2']\n",
    "\n",
    "features = binary_features + ordinal_features + nominal_features + quantitative_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "pipeline = set_pipeline(\n",
    "    model=GradientBoostingClassifier(\n",
    "        loss='log_loss',\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=3,\n",
    "        random_state=random_state,\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors\n",
    ")\n",
    "\n",
    "# Cross-validate the model\n",
    "#scores = cross_val_score(\n",
    "#    pipeline, X_train[features], y_train, cv=5, n_jobs=-1,\n",
    "#    scoring='f1_weighted'\n",
    "#    #scoring='accuracy'\n",
    "#)\n",
    "\n",
    "#print(f'Cross-Validation Scores: {scores}')\n",
    "#print(f'Mean Cross-Validation Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom function to combine text features\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append('../src/features/')\n",
    "\n",
    "import build_features\n",
    "importlib.reload(build_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, label_ranking_average_precision_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join('..', 'data', 'cleaned')\n",
    "\n",
    "# Load the variables dictionary and return the features list\n",
    "varaibles_path = os.path.join(input_path, 'variables.json')\n",
    "features = build_features.load_features(varaibles_path)\n",
    "\n",
    "# Load and clean the REASON FOR VISIT classification summary of codes\n",
    "rfv_path = os.path.join('..', 'data', 'raw', 'RFV_codes_summary.xlsx')\n",
    "rfv_df = build_features.load_rfv(rfv_path)\n",
    "\n",
    "# Load the list of three-digit categories of ICD-9-CM\n",
    "icd9cm_path = os.path.join('..', 'data', 'raw', 'ICD9CM_3DCat.xlsx')\n",
    "icd9cm_df = build_features.load_icd9cm(icd9cm_path)\n",
    "\n",
    "icd9cm_category = 'CATEGORY_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 - Load and prepare the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_df = pd.read_parquet(os.path.join(input_path, 'train.parquet'))\n",
    "\n",
    "X_train, y_train = build_features.build_features(train_df, features, rfv_df, icd9cm_df, category=icd9cm_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'RFV1_TEXT', 'RFV2_TEXT', 'RFV3_TEXT' into 'TEXT'\n",
    "X_train['TEXT'] = X_train['RFV1_TEXT'].fillna('') + ' ' + X_train['RFV2_TEXT'].fillna('') + ' ' + X_train['RFV3_TEXT'].fillna('')\n",
    "\n",
    "# Preprocess the text features with Spacy\n",
    "X_train['TEXT'] = X_train['TEXT'].apply(preprocess_text)\n",
    "\n",
    "# Generate text feature with LDA (Topic probabilities)\n",
    "X_train, vectorizer, tf, lda, topic_features = generate_topic_features(\n",
    "    X_train, n_topics=10, n_top_words=10, transform='log'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heat map of topic distributions among the labels in the training dataset with Altair\n",
    "topic_df = pd.concat(\n",
    "    [X_train[topic_features], y_train], axis=1\n",
    ").rename(columns={0: 'CATEGORY'}).melt(id_vars='CATEGORY', var_name='Topic', value_name='Probability')\n",
    "\n",
    "chart(\n",
    "    df=topic_df,\n",
    "    y='CATEGORY:N',\n",
    "    x='Topic:N',\n",
    "    color='Probability:Q',\n",
    "    title='Distribution of the Labels in the Training Dataset',\n",
    ").mark_rect().configure_axisY(\n",
    "    labelLimit=300, title=None\n",
    ").configure_axisX(\n",
    "    title=None\n",
    ").properties(width=480, height=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution and percentage of the labels in the training dataset\n",
    "y_train.value_counts(normalize=True)\n",
    "\n",
    "# Plot the distribution and percentage of true labels\n",
    "chart(\n",
    "    df=y_train.value_counts(normalize=True).reset_index(),\n",
    "    x='index:N',\n",
    "    y='proportion:Q',\n",
    "    title='Distribution of the Labels in the Training Dataset',\n",
    ").mark_bar().configure_axisX(labelAngle=45, labelLimit=300, title=None).configure_axisY(title=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1.1 - Account for the imbalance in the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Undersampling\n",
    "def random_undersampling(X_train, y_train):\n",
    "    undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=random_state)\n",
    "    X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    return X_train_resampled, y_train_resampled\n",
    "\n",
    "\n",
    "#X_train, y_train = random_undersampling(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1.2 - Confirm the distribution and percentage of the labels in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution and percentage of the labels in the training dataset\n",
    "y_train.value_counts(normalize=True)\n",
    "\n",
    "# Plot the distribution and percentage of true labels\n",
    "chart(\n",
    "    df=y_train.value_counts(normalize=True).reset_index(),\n",
    "    x='index:N',\n",
    "    y='proportion:Q',\n",
    "    title='Distribution of the Labels in the Training Dataset',\n",
    ").mark_bar().configure_axisX(labelAngle=45, labelLimit=300, title=None).configure_axisY(title=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# With original quantitative data and RFV codes + RFV Modules + Topic Features\n",
    "quantitative_features = quantitative_features_wo_bin\n",
    "nominal_features = nominal_features_wo_bin\n",
    "nominal_features = nominal_features + ['RFV1_MOD1', 'RFV2_MOD1', 'RFV3_MOD1'] + ['RFV1_MOD2', 'RFV2_MOD2', 'RFV3_MOD2']\n",
    "\n",
    "clf_features = binary_features + ordinal_features + nominal_features + quantitative_features + topic_features\n",
    "\n",
    "print(f'Missing Values in X_train: \\n{X_train[clf_features].isna().sum().where(lambda x: x > 0).dropna()}')\n",
    "print()\n",
    "print(f'Shape of X_train: {X_train[clf_features].shape}')\n",
    "print(f'Features to be fit: \\n{X_train[clf_features].columns}')\n",
    "print()\n",
    "\n",
    "# Define the model\n",
    "n_neighbors = 5\n",
    "clf_pipeline = set_pipeline(\n",
    "    model=RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        criterion='entropy',\n",
    "        #class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state\n",
    "    ),\n",
    "    binary_features=binary_features,\n",
    "    ordinal_features=ordinal_features,\n",
    "    nominal_features=nominal_features,\n",
    "    quantitative_features=quantitative_features,\n",
    "    imputer='knn',\n",
    "    n_neighbors=n_neighbors,\n",
    "    #pca=True,\n",
    "    #ovr=True\n",
    ")\n",
    "\n",
    "clf_model_name = clf_pipeline.named_steps['classifier'].__class__.__name__\n",
    "if 'one' in clf_model_name.lower():\n",
    "    clf_model_name = f'OneVsRest {type(clf_pipeline.esitmator).__name__}'\n",
    "print(f'Classifier: {clf_model_name}')\n",
    "\n",
    "clf_pipeline.fit(X_train[clf_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 - Load and prepare the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_df = pd.read_parquet(os.path.join(input_path, 'test.parquet'))\n",
    "\n",
    "X_test, y_test = build_features.build_features(test_df, features, rfv_df, icd9cm_df, category=icd9cm_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'RFV1_TEXT', 'RFV2_TEXT', 'RFV3_TEXT' into 'TEXT'\n",
    "X_test['TEXT'] = X_test['RFV1_TEXT'].fillna('') + ' ' + X_test['RFV2_TEXT'].fillna('') + ' ' + X_test['RFV3_TEXT'].fillna('')\n",
    "\n",
    "# Preprocess the text features with Spacy\n",
    "X_test['TEXT'] = X_test['TEXT'].apply(preprocess_text)\n",
    "\n",
    "# Transform the text features with the pre-trained vectorizer and lda\n",
    "test_tf = vectorizer.transform(X_test['TEXT'])\n",
    "test_topics = lda.transform(test_tf)\n",
    "test_topics = pd.DataFrame(test_topics, columns=topic_features, index=X_test.index)\n",
    "\n",
    "X_test = pd.concat([X_test, test_topics], axis=1)\n",
    "print(f'X_test Shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution and percentage of true labels\n",
    "y_test.value_counts(normalize=True)\n",
    "\n",
    "# Plot the distribution and percentage of true labels\n",
    "chart(\n",
    "    df=y_test.value_counts(normalize=True).reset_index(),\n",
    "    x='index:N',\n",
    "    y='proportion:Q',\n",
    "    title='Distribution of True Labels',\n",
    ").mark_bar().configure_axisX(labelAngle=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 - Prediction and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_test_encoded = le.fit_transform(y_test)\n",
    "y_pred_encoded = le.transform(y_pred)\n",
    "\n",
    "n_classes = len(le.classes_)\n",
    "y_test_binarized = label_binarize(y_test_encoded, classes=range(n_classes))\n",
    "y_pred_binarized = label_binarize(y_pred_encoded, classes=range(n_classes))\n",
    "\n",
    "lrap = label_ranking_average_precision_score(y_test_binarized, y_pred_binarized)\n",
    "print(f'Label Ranking Average Precision Score: {lrap}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Prediction Accuracy: {accuracy}')\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f'Weighted Prediction F1 Score: {f1}')\n",
    "\n",
    "print(classification_report(y_test[y_test.notna()], y_pred[y_test.notna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.5 - Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix with percentages\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_percent = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(cm_percent, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=clf_pipeline.classes_, yticklabels=clf_pipeline.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Percentages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Extract text features from each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom function to combine text features\n",
    "import sys\n",
    "sys.path.append('../src/features/')\n",
    "\n",
    "from combine_textual import combine_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 - Aggregate text data by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of textual features to combine\n",
    "textual_features = [\n",
    "    'AGE', 'SEX', 'USETOBAC', \n",
    "    'MAJOR', 'RFV1', 'RFV2', 'RFV3', \n",
    "    'BMI', 'TEMPF', 'BPSYS', 'BPDIAS',\n",
    "    'ARTHRTIS', 'ASTHMA', 'CANCER', 'CEBVD', 'CHF', 'CRF', 'COPD', 'DEPRN', 'DIABETES', 'HYPLIPID', 'HTN', 'IHD', 'OBESITY', 'OSTPRSIS', 'NOCHRON', 'DMP',\n",
    "    'DIAG1', 'DIAG2', 'DIAG3'\n",
    "]\n",
    "\n",
    "# Export the list of textual features\n",
    "with open(os.path.join(file_path, 'textual_features.json'), 'w') as f:\n",
    "    json.dump(textual_features, f)\n",
    "\n",
    "# Combine the text features\n",
    "train_df['CombinedText'] = train_df.apply(lambda x: combine_features(x, textual_features), axis=1)\n",
    "\n",
    "train_df.CombinedText.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.CombinedText.notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 - Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_text(row):\n",
    "    row = re.sub(r'(\\d+),(\\d+)', r'\\1\\2', row)\n",
    "    row = re.sub(r'(\\d+)-(\\d+)', r'\\1_\\2', row)\n",
    "    doc = nlp(row)\n",
    "    processed_text = ' '.join(token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct)\n",
    "    row = re.sub(r'(\\d+)_(\\d+)', r'\\1-\\2', row)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ProcessedText'] = train_df['CombinedText'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.ProcessedText.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed DataFrame\n",
    "processed_file_path = os.path.join('..', 'data', 'processed')\n",
    "train_df.to_csv(os.path.join(processed_file_path, f'train_{clustering_model_name}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 - Calculate term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "# Calculate the TF-IDF for each cluster,\n",
    "# taking the ProcessedText of each cluster as the documents,\n",
    "# and the ProcessedText of the entire dataset as the corpus\n",
    "\n",
    "#clustered_text = train_df.groupby('cluster')['ProcessedText'].apply(lambda row: ' '.join(row)).reset_index()\n",
    "\n",
    "#vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=1000, min_df=5, max_df=0.7)\n",
    "#tfidf_matrix = vectorizer.fit_transform(clustered_text['ProcessedText'])\n",
    "\n",
    "#print(tfidf_matrix)\n",
    "\n",
    "#tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "#tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "# Calculate the TF-IDF of each row within each cluster\n",
    "# Calculate the average TF-IDF for each cluster\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=1000, min_df=5, max_df=0.7)\n",
    "tfidf_matrix = vectorizer.fit_transform(train_df['ProcessedText'])\n",
    "\n",
    "# Calculate the average TF-IDF for each cluster\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "cluster_tfidf = pd.concat([train_df['cluster'], tfidf_df], axis=1).groupby('cluster').mean()\n",
    "\n",
    "cluster_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punish the weight of '\\d+_year_old' by multiplying it by 0.5, using the regex pattern\n",
    "\n",
    "#tfidf_df = tfidf_df.apply(lambda row: row * 0.5 if re.match(r'\\d+_year_old', row.name) else row)\n",
    "#cluster_tfidf = cluster_tfidf.apply(lambda row: row * 0.5 if re.match(r'\\d+_year_old', row.name) else row)\n",
    "\n",
    "#cluster_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 - Generate word clouds for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the word cloud for each cluster basd on the average TF-IDF\n",
    "for i in range(n_clusters):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(cluster_tfidf.loc[i])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Cluster {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Medical pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'services' features\n",
    "binary_services = [feature for feature in variables['services'] if train_df[feature].nunique() <= 2]\n",
    "print(f'Binary Services: {binary_services}')\n",
    "\n",
    "nominal_services = [feature for feature in variables['services'] if feature not in binary_services]\n",
    "print(f'Nominal Services: {nominal_services}')\n",
    "print()\n",
    "\n",
    "# 'medicationsAndImmunizations' features\n",
    "quantitative_med = ['NUMNEW', 'NUMCONT']\n",
    "print(f'Quantitative Medications: {quantitative_med}')\n",
    "\n",
    "binary_med = [feature for feature in variables['medicationsAndImmunizations'] if train_df[feature].nunique() <= 2 and feature not in quantitative_med]\n",
    "print(f'Binary Medications: {binary_med}')\n",
    "\n",
    "nominal_med = [feature for feature in variables['medicationsAndImmunizations'] if feature not in binary_med and feature not in quantitative_med]\n",
    "print(f'Nominal Medications: {nominal_med}')\n",
    "print()\n",
    "\n",
    "# 'providersSeen' features\n",
    "binary_ps = [feature for feature in variables['providersSeen'] if train_df[feature].nunique() <= 2]\n",
    "print(f'Binary Providers Seen: {binary_ps}')\n",
    "print()\n",
    "\n",
    "# 'visitDisposition' features\n",
    "binary_vd = [feature for feature in variables['visitDisposition'] if train_df[feature].nunique() <= 2]\n",
    "print(f'Binary Visit Disposition: {binary_vd}')\n",
    "print()\n",
    "\n",
    "# 'DIAG' features\n",
    "nominal_diag = ['DIAG1', 'DIAG1_CAT1', 'DIAG1_CAT2']\n",
    "print(f'Nominal Diagnosis: {nominal_diag}')\n",
    "text_diag = 'DIAG1_TEXT'\n",
    "print(f'Text Diagnosis: {text_diag}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
